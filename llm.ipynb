{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b018cdb4",
   "metadata": {},
   "source": [
    "1. Packages 설치\n",
    "- `%%capture`\n",
    "> Jupyter Notebook에서 셀의 출력을 숨기는 옵션 (긴 설치 과정 로그 감추기)\n",
    "\n",
    "- Unsloth\n",
    "> LLAMA 모델의 fine-tuning을 효율적으로 쉽게 만들어 주는 도구\n",
    "> `[colab-new]`는 Google Colab 환경에 맞는 버전을 설치하는 옵션\n",
    "\n",
    "```cmd\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "```\n",
    "\n",
    "- fine-tuning 필요한 추가 라이버리\n",
    "> `--no-deps` 옵션은 라이브러리들의 종속성을 설치하지 않도록 설정\n",
    "\n",
    "```cmd\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "```\n",
    "\n",
    "- 종속성 라이브러리\n",
    "> `xformers` : 트랜스포머 모델의 성능을 향상시키는 라이브러리\n",
    "\n",
    "> `trl` : 강화학습을 이용한 언어 모델 훈련을 위한 라이브러리\n",
    "\n",
    "> `peft` : 매개변수 효율적 미세 조정(Parameter-Efficient Fine-Tuning)을 위한 라이브러리\n",
    "\n",
    "> `accelerate` : 딥러닝 모델의 훈련을 가속화 하는 라이브러리\n",
    "\n",
    "> `bitsandbytes` : 모델 양자화를 위한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# import torch\n",
    "major_version = int(torch.__version__.split('.')[0])\n",
    "if major_version >= 8:\n",
    "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
    "else:\n",
    "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3b52e",
   "metadata": {},
   "source": [
    "2. Unsloth 실행\n",
    "- 모델이 처리 할 수 있는 최대 시퀀스 길이 설정\n",
    "> `RoPE(Rotary Position Embedding)` 스케일링을 자동으로 지원하므로 원하는 값으로 설정 가능\n",
    "\n",
    "```python\n",
    "max_seq_length = 2048\n",
    "```\n",
    "\n",
    "- 모델의 데이터 타입을 설정 `None`으로 두면 자동으로 감지\n",
    "> `Tesla T4, V100 GPU`의 경우 `Float16, Ampere` 이상의 GPU에서 `Bfloat16`을 사용\n",
    "\n",
    "```python\n",
    "dtype = None\n",
    "```\n",
    "\n",
    "- 4비트 양자화를 사용할지 설정, 메모리 사용량을 줄이는 데 필요\n",
    "> `True`로 설정하면 4비트 양자화를 사용\n",
    "\n",
    "> `False`로 설정하면 사용하지 않음\n",
    "\n",
    "```python\n",
    "load_in_4bit = True\n",
    "```\n",
    "\n",
    "- 미리 4비트로 양자화된 모델들의 목록\n",
    "> 나열한 모델들은 다운로드가 4배 빠르고 메모리 부족 문제(`OOM`)를 방지\n",
    "\n",
    "```python\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-it-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "]\n",
    "```\n",
    "\n",
    "- `FastLanguageModel`\n",
    "> 사전 훈련된 모델과 토크나이저를 로드\n",
    "\n",
    "```python\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "```\n",
    "\n",
    "> PEFT(부분 파인튜닝) 모델을 생성\n",
    "\n",
    "```python\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # LoRA(Low-Rank Adaptation)의 랭크 값. 작을수록 파라미터 수가 적음.\n",
    "    lora_alpha = 32, # LoRA scaling factor. 모델이 학습하는 속도나 성능에 영향을 줄 수 있음.\n",
    "    lora_dropout = 0.05, # LoRA 학습 시 적용할 드롭아웃 비율. 과적합 방지를 위함.\n",
    "    # LoRA를 적용할 모델 내 모듈들 (주로 attention 및 feed-forward network의 projection layer들)\n",
    "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\",],\n",
    "    bias = \"none\", # bias 항을 학습하지 않음. (\"none\", \"all\", \"lora_only\" 가능)\n",
    "    use_gradient_checkpointing = \"unsloth\", # gradient checkpointing을 통해 메모리 절약. \"unsloth\"는 특정 최적화 방식 사용.\n",
    "    random_state = 123, # 결과 재현성을 위한 랜덤 시드 설정\n",
    "    use_rslora = False, # rslora (rank-stable LoRA) 사용 여부. False면 일반 LoRA 사용\n",
    "    loftq_config = None, # LoFTQ (Low-rank + Quantization) 관련 설정. None이면 미사용\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a1fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "model_name = \"\"\n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-it-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "]\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7090a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.05,\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 123,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0736c694",
   "metadata": {},
   "source": [
    "3. 데이터셋 준비하기\n",
    "- HuggingFace 또는 Local (`jsonl`) 사용하기\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "data_model = \"\"\n",
    "dataset = load_dataset(data_model, split=\"train\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2aa1c2",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "data_model = \"\"\n",
    "dataset = load_dataset(data_model, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f32a71",
   "metadata": {},
   "source": [
    "4. 주어진 예시들을 포맷팅하는 함수 및 데이터셋에 formatting_prompts_func 함수\n",
    "- `Tokenizer`의 EOS(`End of Sequence`) 토큰을 가져오기\n",
    "```python\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "```\n",
    "\n",
    "- `Alpaca` 형식의 프롬프트 템플릿 정의\n",
    "```python\n",
    "alpaca_prompt = \"\"\"\n",
    "\n",
    "### Instruction:\n",
    "{0}\n",
    "\n",
    "### Input:\n",
    "{1}\n",
    "\n",
    "### Response:\n",
    "{2}\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"QUESTION\"]\n",
    "    outputs = examples[\"ANSWER\"]\n",
    "    texts = []\n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        text = alpaca_prompt.format(instruction, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb65a7",
   "metadata": {},
   "source": [
    "5. Huggingface TRL의 SFTTrainer를 사용하여 모델 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e4c4c0",
   "metadata": {},
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d37ce6",
   "metadata": {},
   "source": [
    "6. 모델을 훈련시키고 통계를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813cb9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181c528",
   "metadata": {},
   "source": [
    "7. 모델 실행하기 (추론)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ef3eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id):\n",
    "        self.stop_token_id = stop_token_id\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return (\n",
    "            self.stop_token_id in input_ids[0]\n",
    "        )\n",
    "\n",
    "stop_token = \"<|end_of_text|>\"\n",
    "stop_token_id = tokenizer.encode(stop_token, add_special_tokens=False)[\n",
    "    0\n",
    "]\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList(\n",
    "    [StopOnToken(stop_token_id)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2a4bd",
   "metadata": {},
   "source": [
    "8 테스트 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# FastLanguageModel을 이용하여 추론 속도를 2배 빠르게 설정합니다.\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            \"GPT-4와 GPT-3.5 터보는 'LLM 환각 지수' 평가에서 어떤 성능을 보였습니까?\",\n",
    "            \"\",\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=2048,\n",
    "    stopping_criteria=stopping_criteria\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
